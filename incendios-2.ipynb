{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRABAJO REDES NEURONALES Y APRENDIZAJE ESTADÍSTICO\n",
    "\n",
    "\n",
    "Autores: Miguel Bande Rodríguez y Octavian Rotita Ion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from ipywidgets import interact, IntSlider\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La base de datos se encuentra en formato CSV\n",
    "\n",
    "# Cargamos la base de datos\n",
    "df = pd.read_csv('./fires-all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocesado de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Visualización de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las primeras 5 filas\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos lo que significa cada una de las variables:\n",
    "\n",
    "| Nombre del campo   | Descripción |\n",
    "|--------------------|-------------|\n",
    "| id                | Identificador del incendio |\n",
    "| superficie        | Superficie forestal quemada en hectáreas |\n",
    "| fecha            | Fecha de detección del incendio (formato yyyy-mm-dd) |\n",
    "| lat              | Latitud geográfica del origen del incendio |\n",
    "| lng              | Longitud geográfica del origen del incendio |\n",
    "| latlng_explicit  | Indica si se dispone de coordenadas geográficas del incendio (1) o se han usado las coordenadas del municipio de origen del incendio (0) |\n",
    "| comunidad        | Identificador de la CC.AA. |\n",
    "| provincia        | Identificador de la provincia |\n",
    "| municipio        | Nombre del municipio |\n",
    "| causa           | Identificador de la causa del incendio |\n",
    "| causa_supuesta  | Es ‘1’ si la causa es supuesta, si no en blanco |\n",
    "| causa_desc      | Identificador de la descripción de la causa del incendio |\n",
    "| muertos         | Número de muertos en el incendio |\n",
    "| heridos         | Número de heridos en el incendio |\n",
    "| time_ctrl       | Tiempo transcurrido hasta entrar en fase de control del incendio (en minutos) |\n",
    "| time_ext        | Tiempo transcurrido hasta la extinción del incendio (en minutos) |\n",
    "| personal        | Número de personas que han participado en la extinción del incendio (incluye técnicos, agentes forestales, brigadas, bomberos, voluntarios, guardias civiles y ejército) |\n",
    "| medios          | Número de medios terrestres y aéreos que han participado en la extinción del incendio (incluye autobombas, bulldozers, tractores, aviones y otros) |\n",
    "| gastos          | Gastos de extinción asociados al incendio tal y como figuran en EGIF |\n",
    "| perdidas        | Pérdidas económicas asociadas al incendio tal y como figuran en EGIF |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo una breve exploración de la base de datos, podemos ver que no tenemos información de latitud y longitud hasta el año 1983. Además, a partir de 2017 hay provincias que han dejado de proporcionar información.\n",
    "\n",
    "Vamos a convertir a datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convirtamos la casilla fecha a datetime\n",
    "\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hemos decidido eliminar las inconsistencias relacionadas con la fecha que hemos descrito anteriormente. Es por esto que nos quedaremos con los datos a partir del 1 de enero del 1983 hasta el 31 de diciembre de 2016\n",
    "\n",
    "df = df[(df['fecha'] >= '1983-01-01') & (df['fecha'] <= '2016-12-31')]\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que seguimos teniendo valores missing en latitud y longitud, visualicémolos y decidamos que hacer con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los missings de latitud y longitud\n",
    "\n",
    "df[df['lat'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que dichos valores missing en latitud y en longitud se corresponden con categorías en las cuales municipio se corresponde con: otra provincia, portugal o francia. Almodovar de Monterrey es un municipio que ha pasado por diversos procesos de separación y agregación (en cuanto a terrenos). Como es solamente uno el que no tiene valores en latitud y longitud, eliminémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['lat'].isnull()].index, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la información del municipio la podemos obtener en función de las variables latitud y longitud, vamos a eliminar dicha variable del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminemos la variable municipio\n",
    "\n",
    "df.drop('municipio', axis=1, inplace=True)\n",
    "df.drop('idmunicipio', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiemos ahora las comunidades y las provincias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar veamos la codificación de las comunidades autónomas (estas se encuentran en la base de datos del ministerio de transición ecológica (EGIF))\n",
    "\n",
    "dict_ccaa = {\n",
    "    '1': 'Euskadi',\n",
    "    '2': 'Cataluña',\n",
    "    '3': 'Galicia',\n",
    "    '4': 'Andalucía',\n",
    "    '5': 'Asturias',\n",
    "    '6': 'Cantabria',\n",
    "    '7': 'La Rioja',\n",
    "    '8': 'Murcia',\n",
    "    '9': 'C. Valenciana',\n",
    "    '10': 'Aragón',\n",
    "    '11': 'Castilla la Mancha',\n",
    "    '12': 'Canarias',\n",
    "    '13': 'Navarra',\n",
    "    '14': 'Extremadura',\n",
    "    '15': 'Illes Balears',\n",
    "    '16': 'Madrid',\n",
    "    '17': 'Castilla y León',\n",
    "    '18': 'Ceuta',\n",
    "    '19': 'Melilla',\n",
    "    '99': 'Otro país'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos un value counts de idcomunidad para ver si los datos son consistentes con el mapa. Juntémolos con el diccionario para saber que comunidad autónoma es cada id\n",
    "\n",
    "df['idcomunidad'] = df['idcomunidad'].astype(str)\n",
    "df['comunidad'] = df['idcomunidad'].map(dict_ccaa)\n",
    "\n",
    "df['comunidad'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que no aparece 'Otro país', por lo que el filtrado anterior lo hemos hecho correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos ahora un diccionario para representar las causas de los incendios\n",
    "\n",
    "causas_incendios = {\n",
    "    1: \"Fuego por rayo\",\n",
    "    2: \"Fuego por accidente o negligencia\",\n",
    "    3: \"Fuego por accidente o negligencia\",\n",
    "    4: \"Fuego intencionado\",\n",
    "    5: \"Fuego por causa desconocida\",\n",
    "    6: \"Incendio reproducido\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución geográfica de los incendios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer el año de la fecha\n",
    "df[\"año\"] = df[\"fecha\"].dt.year\n",
    "colores = ['yellow', 'blue', 'blue', 'red', 'purple', 'orange']\n",
    "\n",
    "# Obtener rango de años disponibles\n",
    "min_año, max_año = df[\"año\"].min(), df[\"año\"].max()\n",
    "\n",
    "def mostrar_mapa(año):\n",
    "    # Filtrar los datos por el año seleccionado\n",
    "    df_filtrado = df[df[\"año\"] == año]\n",
    "\n",
    "    # Crear un mapa centrado en España\n",
    "    mapa = folium.Map(location=[40.0, -3.5], zoom_start=5)\n",
    "\n",
    "    # Agregar los puntos al mapa\n",
    "    for _, row in df_filtrado.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\"lat\"], row[\"lng\"]],\n",
    "            radius=(row[\"superficie\"]-20)/215,\n",
    "            color=colores[row[\"causa\"]-1],\n",
    "            fill=True,\n",
    "            fill_color= colores[row[\"causa\"]-1],\n",
    "            fill_opacity=0.5,\n",
    "        ).add_to(mapa)\n",
    "\n",
    "    return mapa\n",
    "\n",
    "# Crear el widget de selección de año\n",
    "interact(mostrar_mapa, año=IntSlider(min=min_año, max=max_año, step=1, value=min_año))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los datos por el año seleccionado\n",
    "df_filtrado = df[df[\"año\"] == 1983]\n",
    "\n",
    "# Crear un mapa centrado en España\n",
    "mapa = folium.Map(location=[40.0, -3.5], zoom_start=5)\n",
    "\n",
    "# Agregar los puntos al mapa\n",
    "for _, row in df_filtrado.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"lat\"], row[\"lng\"]],\n",
    "        radius=(row[\"superficie\"]-20)/215,\n",
    "        color=colores[row[\"causa\"]-1],\n",
    "        fill=True,\n",
    "        fill_color= colores[row[\"causa\"]-1],\n",
    "        fill_opacity=0.5,\n",
    "    ).add_to(mapa)\n",
    "\n",
    "mapa.save(\"mapa_interactivo.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de algunos histogramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número de incendios por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incendios_por_año = df[\"año\"].value_counts().reset_index()\n",
    "incendios_por_año.columns = [\"Año\", \"Cantidad de Incendios\"]\n",
    "incendios_por_año = incendios_por_año.sort_values(\"Año\")\n",
    "\n",
    "fig = px.bar(\n",
    "    incendios_por_año, \n",
    "    x=\"Año\", \n",
    "    y=\"Cantidad de Incendios\", \n",
    "    title=\"Número de Incendios en España por Año\",\n",
    "    labels={\"Año\": \"Año\", \"Cantidad de Incendios\": \"Número de Incendios\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de superficie quemada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df, x=\"superficie\", nbins=40, \n",
    "    title=\"Distribución de Superficie Quemada\",\n",
    "    labels={\"superficie\": \"Superficie quemada (ha)\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de incendios por mes y año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Agrupar por año y mes\n",
    "df[\"mes\"] = df[\"fecha\"].dt.month\n",
    "heatmap_data = df.groupby([\"año\", \"mes\"]).size().reset_index(name=\"incendios\")\n",
    "\n",
    "fig = px.density_heatmap(\n",
    "    heatmap_data, x=\"mes\", y=\"año\", z=\"incendios\",\n",
    "    title=\"Incendios por Mes y Año\",\n",
    "    labels={\"mes\": \"Mes\", \"año\": \"Año\", \"incendios\": \"Cantidad\"},\n",
    "    color_continuous_scale=\"reds\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relación entre tiempo de extinción y superficie quemada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df, y=\"superficie\", x = \"time_ext\",\n",
    "    title=\"Relación entre Tiempo de Extinción y Superficie Quemada\",\n",
    "    labels={\"time_ext\": \"Tiempo de Extinción (min)\", \"superficie\": \"Superficie Quemada (ha)\"},\n",
    "    log_y=True,\n",
    "    log_x=True\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df, y=\"superficie\", x = \"time_ctrl\",\n",
    "    title=\"Relación entre Tiempo de Control y Superficie Quemada\",\n",
    "    labels={\"time_ext\": \"Tiempo de Control (min)\", \"superficie\": \"Superficie Quemada (ha)\"},\n",
    "    log_y=True,\n",
    "    log_x=True\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear las causas al nombre correspondiente\n",
    "df[\"causa_map\"] = df[\"causa\"].map(causas_incendios)\n",
    "\n",
    "# Contar los incendios por causa\n",
    "causas_contadas = df[\"causa_map\"].value_counts().reset_index()\n",
    "causas_contadas.columns = [\"Causa\", \"Cantidad\"]\n",
    "\n",
    "# Crear gráfico de pastel\n",
    "fig = px.pie(\n",
    "    causas_contadas, \n",
    "    names=\"Causa\", \n",
    "    values=\"Cantidad\",\n",
    "    title=\"Distribución de Causas de Incendios\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticos de las variables presentes en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que hay varios casos con pérdidas negativas. Esto no tiene sentido alguno. Vamos a eliminar estas inconsistencias.\n",
    "\n",
    "df = df[df['perdidas'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiempo promedio de extinción de incendios por comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que la columna \"comunidad\" es categórica y \"time_ext\" es numérico\n",
    "df[\"comunidad\"] = df[\"comunidad\"].astype(str)\n",
    "df[\"time_ext\"] = pd.to_numeric(df[\"time_ext\"], errors=\"coerce\")\n",
    "\n",
    "# Calcular el tiempo medio de extinción por comunidad\n",
    "extincion_por_comunidad = df.groupby(\"comunidad\")[\"time_ext\"].mean().reset_index()\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "fig = px.bar(\n",
    "    extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"time_ext\",\n",
    "    title=\"Tiempo Promedio de Extinción de Incendios por Comunidad\",\n",
    "    labels={\"comunidad\": \"Comunidad Autónoma\", \"time_ext\": \"Tiempo de Extinción (min)\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superficie media quemada por comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que la columna \"comunidad\" es categórica y \"time_ext\" es numérico\n",
    "df[\"comunidad\"] = df[\"comunidad\"].astype(str)\n",
    "\n",
    "# Calcular la superficie media quemada por comunidad\n",
    "extincion_por_comunidad = df.groupby(\"comunidad\")[\"superficie\"].mean().reset_index()\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "fig = px.bar(\n",
    "    extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"superficie\",\n",
    "    title=\"Superficie media quemada por Comunidad\",\n",
    "    labels={\"comunidad\": \"Comunidad Autónoma\", \"time_ext\": \"Tiempo de Extinción (min)\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar el orden de las comunidades\n",
    "orden_comunidades = [\n",
    "    \"C. Valenciana\", \"Aragón\", \"Cantabria\", \"Cataluña\", \"Asturias\", \"La Rioja\", \n",
    "    \"Euskadi\", \"Andalucía\", \"Galicia\", \"Castilla y León\", \"Castilla la Mancha\", \n",
    "    \"Canarias\", \"Illes Balears\", \"Extremadura\", \"Murcia\", \"Madrid\", \"Navarra\", \"Ceuta\"\n",
    "]\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.barplot(\n",
    "    data=extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"superficie\",\n",
    "    order=orden_comunidades\n",
    ")\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.xlabel(\"Comunidad Autónoma\")\n",
    "plt.ylabel(\"Superficie Media Quemada\")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superficie total quemada por Comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que la columna \"comunidad\" es categórica y \"time_ext\" es numérico\n",
    "df[\"comunidad\"] = df[\"comunidad\"].astype(str)\n",
    "\n",
    "# Calcular la superficie total quemada por comunidad\n",
    "extincion_por_comunidad = df.groupby(\"comunidad\")[\"superficie\"].sum().reset_index()\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "fig = px.bar(\n",
    "    extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"superficie\",\n",
    "    title=\"Superficie total quemada por Comunidad\",\n",
    "    labels={\"comunidad\": \"Comunidad Autónoma\", \"time_ext\": \"Tiempo de Extinción (min)\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variables a analizar con boxplots\n",
    "variables_boxplot = [\"superficie\", \"time_ext\", \"time_ctrl\", \"muertos\", \"heridos\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "\n",
    "# Crear gráficos de boxplot para cada variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(20, 4))\n",
    "fig.suptitle(\"Detección de Outliers en Variables Clave\", fontsize=16)\n",
    "\n",
    "# Dibujar cada boxplot\n",
    "for ax, variable in zip(axes.flatten(), variables_boxplot):\n",
    "    sns.boxplot(y=df[variable], ax=ax)\n",
    "\n",
    "# Ajustar el layout para mejor visualización\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a transformar\n",
    "cols_to_transform = [\"superficie\", \"time_ext\", \"time_ctrl\", \"muertos\", \"heridos\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "\n",
    "\n",
    "# Aplicar la transformación log(1 + x)\n",
    "df[cols_to_transform] = df[cols_to_transform].apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación box-plot para detección de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variables a analizar con boxplots\n",
    "variables_boxplot = [\"superficie\", \"time_ext\", \"time_ctrl\", \"muertos\", \"heridos\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "\n",
    "# Crear gráficos de boxplot para cada variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(20, 4))\n",
    "fig.suptitle(\"Detección de Outliers en Variables Clave\", fontsize=16)\n",
    "\n",
    "# Dibujar cada boxplot\n",
    "for ax, variable in zip(axes.flatten(), variables_boxplot):\n",
    "    sns.boxplot(y=df[variable], ax=ax)\n",
    "\n",
    "# Ajustar el layout para mejor visualización\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los diagramas de caja y bigote de superficie (logaritmo) por causa\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x=\"causa_map\", y=\"superficie\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los diagramas de caja y bigote de superficie (logaritmo) por causa\n",
    "\n",
    "plt.figure(figsize=(21, 9))\n",
    "sns.boxplot(data=df, x=\"comunidad\", y=\"superficie\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Selección de variables predictoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo columnas de interés para el análisis de correlación\n",
    "variables_corr = [\"superficie\", \"time_ext\", \"time_ctrl\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "df_numeric = df[variables_corr].apply(pd.to_numeric, errors=\"coerce\")  # Convertir a numérico y forzar errores a NaN\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "# Crear el heatmap de correlación\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Matriz de Correlación entre Variables de Incendios\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las variables más correlacionadas entre sí son el tiempo de control y el tiempo de extinción, pero no están altamente correlacionadas, por lo que decidimos usar ambas variables para entrenamiento de modelo. Además, conocer el tiempo de extinción y el tiempo de contro simultáneamente nos puede indicar la magnitud de superficie quemada, ya que un incendio que ha tardado más tiempo en extinguirse desde el momento que se considera controlado ha podido ser más devastador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia de variables con modelos basados en árboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables predictoras y objetivo\n",
    "X = df[[\"time_ext\", \"time_ctrl\", \"personal\", \"gastos\", \"perdidas\"]]\n",
    "y = df[\"superficie\"]\n",
    "\n",
    "# Entrenar modelo Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Extraer importancia de variables\n",
    "importances = pd.DataFrame({\"Variable\": X.columns, \"Importancia\": rf.feature_importances_})\n",
    "importances = importances.sort_values(by=\"Importancia\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(importances[\"Variable\"], importances[\"Importancia\"], color=\"red\", alpha = 0.5, edgecolor=\"black\")\n",
    "plt.title(\"Importancia de Variables en el Modelo\")\n",
    "plt.xlabel(\"Variable\")\n",
    "plt.ylabel(\"Importancia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizar pruebas estadísticas para seleccionar las mejores características con K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar prueba estadística\n",
    "selector = SelectKBest(score_func=f_regression, k=3)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Ver qué variables fueron seleccionadas\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Entrenamiento red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 División del conjunto en entrenamiento y test (con el fin de evaluar nuestro regresor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ello, vamos a usar la librería de sklearn train_test_split.\n",
    "\n",
    "# En primer lugar nos quedamos con las variables predictoras y la variable objetivo\n",
    "\n",
    "X = df[['time_ext', 'time_ctrl', 'personal', 'perdidas', 'idcomunidad']]\n",
    "# X = df[['idcomunidad','causa','time_ext', 'time_ctrl', 'personal', 'gastos', 'perdidas']]\n",
    "y = df['superficie']\n",
    "\n",
    "# Dividimos los datos en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Codificación de las variables categóricas y normalización de las variables numéricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categóricas y numéricas\n",
    "categorical_features = ['idcomunidad']#['causa']#, 'idcomunidad']\n",
    "numerical_features = ['time_ext', 'time_ctrl', 'personal', 'perdidas']\n",
    "\n",
    "# Crear pipeline con One-Hot Encoding y Normalización\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),  # Normalizar variables numéricas\n",
    "    ('cat', OneHotEncoder(drop='first'), categorical_features)  # One-Hot Encoding en categóricas\n",
    "])\n",
    "\n",
    "# Transformar los datos\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape, X_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Uso de cross-validation para calcular los hiperparámetros óptimos de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la red neuronal base\n",
    "mlp = MLPRegressor(max_iter=500, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a optimizar\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(24,), (32,), (32, 16)],  # Diferentes arquitecturas\n",
    "    'activation': ['relu', 'tanh'],  # Funciones de activación\n",
    "    'solver': ['adam'],  # Algoritmos de optimización\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # Regularización L2 (distribución uniforme)\n",
    "    'learning_rate': ['constant', 'adaptive'],  # Tipo de tasa de aprendizaje\n",
    "}\n",
    "\n",
    "\n",
    "# Ejecutar la búsqueda con Cross-Validation\n",
    "grid_search = GridSearchCV(mlp, \n",
    "                            param_grid,\n",
    "                             cv=5, # Cross-validation con 5 folds\n",
    "                             scoring='neg_mean_squared_error',  # Minimizar MAE\n",
    "                             n_jobs=-1, # Usar todos los núcleos disponibles\n",
    "                             verbose=2) # Mostrar progreso\n",
    "\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Imprimir los mejores parámetros encontrados\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(X_test_transformed)\n",
    "\n",
    "# Evaluación\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"📉 MAE: {mae:.2f} hectáreas\")\n",
    "print(f\"📉 MSE: {mse:.2f} hectáreas²\")\n",
    "print(f\"📊 R² Score: {r2:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a entrenar una red neuronal sola\n",
    "\n",
    "# Crear la red neuronal\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(32,16),  # 2 capas ocultas con 64 y 32 neuronas\n",
    "                    activation='tanh',  # Función de activación ReLU\n",
    "                    solver='adam',  # Optimizador Adam\n",
    "                    alpha=0.0001,  # Regularización L2\n",
    "                    learning_rate='constant',  # Ajuste dinámico de tasa de aprendizaje\n",
    "                    max_iter=500,  # Número de iteraciones de entrenamiento\n",
    "                    random_state=42)\n",
    "\n",
    "# Entrenar la red\n",
    "mlp.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predicciones en datos de prueba\n",
    "y_pred = mlp.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Evaluación del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"📉 MAE (Error Absoluto Medio): {mae:.2f} hectáreas\")\n",
    "print(f\"📉 MSE (Error Cuadrático Medio): {mse:.2f} hectáreas²\")\n",
    "print(f\"📊 R² Score (Precisión del modelo): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veamos las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.element import Template, MacroElement\n",
    "\n",
    "\n",
    "color = ['yellow', 'blue', 'blue', 'red', 'purple', 'orange']\n",
    "# Definir umbral de error (en hectáreas)\n",
    "umbral_error = 0  # Modificar según necesidad\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'Real': y_test,\n",
    "    'Predicho': y_pred\n",
    "})\n",
    "\n",
    "# Calcular el error absoluto\n",
    "resultados['Error'] = np.abs(resultados['Real'] - resultados['Predicho'])\n",
    "\n",
    "resultados['id'] = X_test.index  # Asigna el índice de X_test como identificador\n",
    "df['id'] = df.index  # Si df tampoco tiene id, usa su índice\n",
    "\n",
    "# 📌 Asegurar que df tiene 'id', 'lat', y 'lng' y hacer la unión correctamente\n",
    "resultados = resultados.merge(df, on='id', how='left')  # Cambia 'id' si el nombre es distinto\n",
    "\n",
    "# Filtrar puntos con error superior al umbral\n",
    "errores_significativos = resultados[resultados['Error'] > umbral_error]\n",
    "\n",
    "# Solución: Eliminar NaN antes de usar Folium\n",
    "errores_significativos = errores_significativos.dropna(subset=['lat', 'lng'])\n",
    "\n",
    "# Crear mapa centrado en la media de los puntos\n",
    "mapa = folium.Map(location=[errores_significativos['lat'].mean(), \n",
    "                            errores_significativos['lng'].mean()], zoom_start=6)\n",
    "\n",
    "\n",
    "# Agrupar los marcadores\n",
    "marker_cluster = MarkerCluster().add_to(mapa)\n",
    "\n",
    "\n",
    "# Agregar puntos al mapa\n",
    "for _, fila in errores_significativos.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[fila['lat'], fila['lng']],\n",
    "        radius=fila['superficie'],\n",
    "        color=color[fila[\"causa\"]-1],\n",
    "        fill=True,\n",
    "        fill_color=color[fila[\"causa\"]-1],\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Real: {np.exp(fila['Real'])-1:.2f} ha \\n Predicho: {np.exp(fila['Predicho'])-1:.2f} ha \\n Error: {fila['Error']:.2f} ha \\n Causa: {fila['causa_map']}\"\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "# Mostrar el mapa interactivo\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa.save(\"mapa_interactivo_predicciones.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapa interactivo de los incendios cuya predicción difiere 19 hectáreas del valor real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.element import Template, MacroElement\n",
    "\n",
    "\n",
    "color = ['yellow', 'blue', 'blue', 'red', 'purple', 'orange']\n",
    "# Definir umbral de error (en hectáreas)\n",
    "umbral_error = 3  # Modificar según necesidad\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'Real': y_test,\n",
    "    'Predicho': y_pred\n",
    "})\n",
    "\n",
    "# Calcular el error absoluto\n",
    "resultados['Error'] = np.abs(resultados['Real'] - resultados['Predicho'])\n",
    "\n",
    "resultados['id'] = X_test.index  # Asigna el índice de X_test como identificador\n",
    "df['id'] = df.index  # Si df tampoco tiene id, usa su índice\n",
    "\n",
    "# 📌 Asegurar que df tiene 'id', 'lat', y 'lng' y hacer la unión correctamente\n",
    "resultados = resultados.merge(df, on='id', how='left')  # Cambia 'id' si el nombre es distinto\n",
    "\n",
    "# Filtrar puntos con error superior al umbral\n",
    "errores_significativos = resultados[resultados['Error'] > umbral_error]\n",
    "\n",
    "# Solución: Eliminar NaN antes de usar Folium\n",
    "errores_significativos = errores_significativos.dropna(subset=['lat', 'lng'])\n",
    "\n",
    "# Crear mapa centrado en la media de los puntos\n",
    "mapa = folium.Map(location=[errores_significativos['lat'].mean(), \n",
    "                            errores_significativos['lng'].mean()], zoom_start=6)\n",
    "\n",
    "\n",
    "# Agrupar los marcadores\n",
    "marker_cluster = MarkerCluster().add_to(mapa)\n",
    "\n",
    "\n",
    "# Agregar puntos al mapa\n",
    "for _, fila in errores_significativos.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[fila['lat'], fila['lng']],\n",
    "        radius=fila['superficie'],\n",
    "        color=color[fila[\"causa\"]-1],\n",
    "        fill=True,\n",
    "        fill_color=color[fila[\"causa\"]-1],\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Real: {np.exp(fila['Real'])-1:.2f} ha \\n Predicho: {np.exp(fila['Predicho'])-1:.2f} ha \\n Error: {fila['Error']:.2f} ha \\n Causa: {fila['causa_map']}\"\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "# Mostrar el mapa interactivo\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa.save(\"mapa_interactivo_predicciones_3.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

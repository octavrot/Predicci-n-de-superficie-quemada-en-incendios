{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRABAJO REDES NEURONALES Y APRENDIZAJE ESTAD√çSTICO\n",
    "\n",
    "\n",
    "Autores: Miguel Bande Rodr√≠guez y Octavian Rotita Ion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos las librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from ipywidgets import interact, IntSlider\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La base de datos se encuentra en formato CSV\n",
    "\n",
    "# Cargamos la base de datos\n",
    "df = pd.read_csv('./fires-all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocesado de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Visualizaci√≥n de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las primeras 5 filas\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos lo que significa cada una de las variables:\n",
    "\n",
    "| Nombre del campo   | Descripci√≥n |\n",
    "|--------------------|-------------|\n",
    "| id                | Identificador del incendio |\n",
    "| superficie        | Superficie forestal quemada en hect√°reas |\n",
    "| fecha            | Fecha de detecci√≥n del incendio (formato yyyy-mm-dd) |\n",
    "| lat              | Latitud geogr√°fica del origen del incendio |\n",
    "| lng              | Longitud geogr√°fica del origen del incendio |\n",
    "| latlng_explicit  | Indica si se dispone de coordenadas geogr√°ficas del incendio (1) o se han usado las coordenadas del municipio de origen del incendio (0) |\n",
    "| comunidad        | Identificador de la CC.AA. |\n",
    "| provincia        | Identificador de la provincia |\n",
    "| municipio        | Nombre del municipio |\n",
    "| causa           | Identificador de la causa del incendio |\n",
    "| causa_supuesta  | Es ‚Äò1‚Äô si la causa es supuesta, si no en blanco |\n",
    "| causa_desc      | Identificador de la descripci√≥n de la causa del incendio |\n",
    "| muertos         | N√∫mero de muertos en el incendio |\n",
    "| heridos         | N√∫mero de heridos en el incendio |\n",
    "| time_ctrl       | Tiempo transcurrido hasta entrar en fase de control del incendio (en minutos) |\n",
    "| time_ext        | Tiempo transcurrido hasta la extinci√≥n del incendio (en minutos) |\n",
    "| personal        | N√∫mero de personas que han participado en la extinci√≥n del incendio (incluye t√©cnicos, agentes forestales, brigadas, bomberos, voluntarios, guardias civiles y ej√©rcito) |\n",
    "| medios          | N√∫mero de medios terrestres y a√©reos que han participado en la extinci√≥n del incendio (incluye autobombas, bulldozers, tractores, aviones y otros) |\n",
    "| gastos          | Gastos de extinci√≥n asociados al incendio tal y como figuran en EGIF |\n",
    "| perdidas        | P√©rdidas econ√≥micas asociadas al incendio tal y como figuran en EGIF |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo una breve exploraci√≥n de la base de datos, podemos ver que no tenemos informaci√≥n de latitud y longitud hasta el a√±o 1983. Adem√°s, a partir de 2017 hay provincias que han dejado de proporcionar informaci√≥n.\n",
    "\n",
    "Vamos a convertir a datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convirtamos la casilla fecha a datetime\n",
    "\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hemos decidido eliminar las inconsistencias relacionadas con la fecha que hemos descrito anteriormente. Es por esto que nos quedaremos con los datos a partir del 1 de enero del 1983 hasta el 31 de diciembre de 2016\n",
    "\n",
    "df = df[(df['fecha'] >= '1983-01-01') & (df['fecha'] <= '2016-12-31')]\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que seguimos teniendo valores missing en latitud y longitud, visualic√©molos y decidamos que hacer con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los missings de latitud y longitud\n",
    "\n",
    "df[df['lat'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que dichos valores missing en latitud y en longitud se corresponden con categor√≠as en las cuales municipio se corresponde con: otra provincia, portugal o francia. Almodovar de Monterrey es un municipio que ha pasado por diversos procesos de separaci√≥n y agregaci√≥n (en cuanto a terrenos). Como es solamente uno el que no tiene valores en latitud y longitud, elimin√©moslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['lat'].isnull()].index, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la informaci√≥n del municipio la podemos obtener en funci√≥n de las variables latitud y longitud, vamos a eliminar dicha variable del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminemos la variable municipio\n",
    "\n",
    "df.drop('municipio', axis=1, inplace=True)\n",
    "df.drop('idmunicipio', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiemos ahora las comunidades y las provincias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar veamos la codificaci√≥n de las comunidades aut√≥nomas (estas se encuentran en la base de datos del ministerio de transici√≥n ecol√≥gica (EGIF))\n",
    "\n",
    "dict_ccaa = {\n",
    "    '1': 'Euskadi',\n",
    "    '2': 'Catalu√±a',\n",
    "    '3': 'Galicia',\n",
    "    '4': 'Andaluc√≠a',\n",
    "    '5': 'Asturias',\n",
    "    '6': 'Cantabria',\n",
    "    '7': 'La Rioja',\n",
    "    '8': 'Murcia',\n",
    "    '9': 'C. Valenciana',\n",
    "    '10': 'Arag√≥n',\n",
    "    '11': 'Castilla la Mancha',\n",
    "    '12': 'Canarias',\n",
    "    '13': 'Navarra',\n",
    "    '14': 'Extremadura',\n",
    "    '15': 'Illes Balears',\n",
    "    '16': 'Madrid',\n",
    "    '17': 'Castilla y Le√≥n',\n",
    "    '18': 'Ceuta',\n",
    "    '19': 'Melilla',\n",
    "    '99': 'Otro pa√≠s'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos un value counts de idcomunidad para ver si los datos son consistentes con el mapa. Junt√©molos con el diccionario para saber que comunidad aut√≥noma es cada id\n",
    "\n",
    "df['idcomunidad'] = df['idcomunidad'].astype(str)\n",
    "df['comunidad'] = df['idcomunidad'].map(dict_ccaa)\n",
    "\n",
    "df['comunidad'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que no aparece 'Otro pa√≠s', por lo que el filtrado anterior lo hemos hecho correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos ahora un diccionario para representar las causas de los incendios\n",
    "\n",
    "causas_incendios = {\n",
    "    1: \"Fuego por rayo\",\n",
    "    2: \"Fuego por accidente o negligencia\",\n",
    "    3: \"Fuego por accidente o negligencia\",\n",
    "    4: \"Fuego intencionado\",\n",
    "    5: \"Fuego por causa desconocida\",\n",
    "    6: \"Incendio reproducido\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuci√≥n geogr√°fica de los incendios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer el a√±o de la fecha\n",
    "df[\"a√±o\"] = df[\"fecha\"].dt.year\n",
    "colores = ['yellow', 'blue', 'blue', 'red', 'purple', 'orange']\n",
    "\n",
    "# Obtener rango de a√±os disponibles\n",
    "min_a√±o, max_a√±o = df[\"a√±o\"].min(), df[\"a√±o\"].max()\n",
    "\n",
    "def mostrar_mapa(a√±o):\n",
    "    # Filtrar los datos por el a√±o seleccionado\n",
    "    df_filtrado = df[df[\"a√±o\"] == a√±o]\n",
    "\n",
    "    # Crear un mapa centrado en Espa√±a\n",
    "    mapa = folium.Map(location=[40.0, -3.5], zoom_start=5)\n",
    "\n",
    "    # Agregar los puntos al mapa\n",
    "    for _, row in df_filtrado.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\"lat\"], row[\"lng\"]],\n",
    "            radius=(row[\"superficie\"]-20)/215,\n",
    "            color=colores[row[\"causa\"]-1],\n",
    "            fill=True,\n",
    "            fill_color= colores[row[\"causa\"]-1],\n",
    "            fill_opacity=0.5,\n",
    "        ).add_to(mapa)\n",
    "\n",
    "    return mapa\n",
    "\n",
    "# Crear el widget de selecci√≥n de a√±o\n",
    "interact(mostrar_mapa, a√±o=IntSlider(min=min_a√±o, max=max_a√±o, step=1, value=min_a√±o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los datos por el a√±o seleccionado\n",
    "df_filtrado = df[df[\"a√±o\"] == 1983]\n",
    "\n",
    "# Crear un mapa centrado en Espa√±a\n",
    "mapa = folium.Map(location=[40.0, -3.5], zoom_start=5)\n",
    "\n",
    "# Agregar los puntos al mapa\n",
    "for _, row in df_filtrado.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"lat\"], row[\"lng\"]],\n",
    "        radius=(row[\"superficie\"]-20)/215,\n",
    "        color=colores[row[\"causa\"]-1],\n",
    "        fill=True,\n",
    "        fill_color= colores[row[\"causa\"]-1],\n",
    "        fill_opacity=0.5,\n",
    "    ).add_to(mapa)\n",
    "\n",
    "mapa.save(\"mapa_interactivo.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizaci√≥n de algunos histogramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N√∫mero de incendios por a√±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incendios_por_a√±o = df[\"a√±o\"].value_counts().reset_index()\n",
    "incendios_por_a√±o.columns = [\"A√±o\", \"Cantidad de Incendios\"]\n",
    "incendios_por_a√±o = incendios_por_a√±o.sort_values(\"A√±o\")\n",
    "\n",
    "fig = px.bar(\n",
    "    incendios_por_a√±o, \n",
    "    x=\"A√±o\", \n",
    "    y=\"Cantidad de Incendios\", \n",
    "    title=\"N√∫mero de Incendios en Espa√±a por A√±o\",\n",
    "    labels={\"A√±o\": \"A√±o\", \"Cantidad de Incendios\": \"N√∫mero de Incendios\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuci√≥n de superficie quemada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df, x=\"superficie\", nbins=40, \n",
    "    title=\"Distribuci√≥n de Superficie Quemada\",\n",
    "    labels={\"superficie\": \"Superficie quemada (ha)\"},\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuci√≥n de incendios por mes y a√±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Agrupar por a√±o y mes\n",
    "df[\"mes\"] = df[\"fecha\"].dt.month\n",
    "heatmap_data = df.groupby([\"a√±o\", \"mes\"]).size().reset_index(name=\"incendios\")\n",
    "\n",
    "fig = px.density_heatmap(\n",
    "    heatmap_data, x=\"mes\", y=\"a√±o\", z=\"incendios\",\n",
    "    title=\"Incendios por Mes y A√±o\",\n",
    "    labels={\"mes\": \"Mes\", \"a√±o\": \"A√±o\", \"incendios\": \"Cantidad\"},\n",
    "    color_continuous_scale=\"reds\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaci√≥n entre tiempo de extinci√≥n y superficie quemada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df, y=\"superficie\", x = \"time_ext\",\n",
    "    title=\"Relaci√≥n entre Tiempo de Extinci√≥n y Superficie Quemada\",\n",
    "    labels={\"time_ext\": \"Tiempo de Extinci√≥n (min)\", \"superficie\": \"Superficie Quemada (ha)\"},\n",
    "    log_y=True,\n",
    "    log_x=True\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df, y=\"superficie\", x = \"time_ctrl\",\n",
    "    title=\"Relaci√≥n entre Tiempo de Control y Superficie Quemada\",\n",
    "    labels={\"time_ext\": \"Tiempo de Control (min)\", \"superficie\": \"Superficie Quemada (ha)\"},\n",
    "    log_y=True,\n",
    "    log_x=True\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear las causas al nombre correspondiente\n",
    "df[\"causa_map\"] = df[\"causa\"].map(causas_incendios)\n",
    "\n",
    "# Contar los incendios por causa\n",
    "causas_contadas = df[\"causa_map\"].value_counts().reset_index()\n",
    "causas_contadas.columns = [\"Causa\", \"Cantidad\"]\n",
    "\n",
    "# Crear gr√°fico de pastel\n",
    "fig = px.pie(\n",
    "    causas_contadas, \n",
    "    names=\"Causa\", \n",
    "    values=\"Cantidad\",\n",
    "    title=\"Distribuci√≥n de Causas de Incendios\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estad√≠sticos de las variables presentes en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que hay varios casos con p√©rdidas negativas. Esto no tiene sentido alguno. Vamos a eliminar estas inconsistencias.\n",
    "\n",
    "df = df[df['perdidas'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiempo promedio de extinci√≥n de incendios por comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que la columna \"comunidad\" es categ√≥rica y \"time_ext\" es num√©rico\n",
    "df[\"comunidad\"] = df[\"comunidad\"].astype(str)\n",
    "df[\"time_ext\"] = pd.to_numeric(df[\"time_ext\"], errors=\"coerce\")\n",
    "\n",
    "# Calcular el tiempo medio de extinci√≥n por comunidad\n",
    "extincion_por_comunidad = df.groupby(\"comunidad\")[\"time_ext\"].mean().reset_index()\n",
    "\n",
    "# Crear el gr√°fico de barras\n",
    "fig = px.bar(\n",
    "    extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"time_ext\",\n",
    "    title=\"Tiempo Promedio de Extinci√≥n de Incendios por Comunidad\",\n",
    "    labels={\"comunidad\": \"Comunidad Aut√≥noma\", \"time_ext\": \"Tiempo de Extinci√≥n (min)\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superficie media quemada por comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que la columna \"comunidad\" es categ√≥rica y \"time_ext\" es num√©rico\n",
    "df[\"comunidad\"] = df[\"comunidad\"].astype(str)\n",
    "\n",
    "# Calcular la superficie media quemada por comunidad\n",
    "extincion_por_comunidad = df.groupby(\"comunidad\")[\"superficie\"].mean().reset_index()\n",
    "\n",
    "# Crear el gr√°fico de barras\n",
    "fig = px.bar(\n",
    "    extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"superficie\",\n",
    "    title=\"Superficie media quemada por Comunidad\",\n",
    "    labels={\"comunidad\": \"Comunidad Aut√≥noma\", \"time_ext\": \"Tiempo de Extinci√≥n (min)\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar el orden de las comunidades\n",
    "orden_comunidades = [\n",
    "    \"C. Valenciana\", \"Arag√≥n\", \"Cantabria\", \"Catalu√±a\", \"Asturias\", \"La Rioja\", \n",
    "    \"Euskadi\", \"Andaluc√≠a\", \"Galicia\", \"Castilla y Le√≥n\", \"Castilla la Mancha\", \n",
    "    \"Canarias\", \"Illes Balears\", \"Extremadura\", \"Murcia\", \"Madrid\", \"Navarra\", \"Ceuta\"\n",
    "]\n",
    "\n",
    "# Crear el gr√°fico de barras\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.barplot(\n",
    "    data=extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"superficie\",\n",
    "    order=orden_comunidades\n",
    ")\n",
    "\n",
    "# A√±adir t√≠tulo y etiquetas\n",
    "plt.xlabel(\"Comunidad Aut√≥noma\")\n",
    "plt.ylabel(\"Superficie Media Quemada\")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superficie total quemada por Comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que la columna \"comunidad\" es categ√≥rica y \"time_ext\" es num√©rico\n",
    "df[\"comunidad\"] = df[\"comunidad\"].astype(str)\n",
    "\n",
    "# Calcular la superficie total quemada por comunidad\n",
    "extincion_por_comunidad = df.groupby(\"comunidad\")[\"superficie\"].sum().reset_index()\n",
    "\n",
    "# Crear el gr√°fico de barras\n",
    "fig = px.bar(\n",
    "    extincion_por_comunidad,\n",
    "    x=\"comunidad\",\n",
    "    y=\"superficie\",\n",
    "    title=\"Superficie total quemada por Comunidad\",\n",
    "    labels={\"comunidad\": \"Comunidad Aut√≥noma\", \"time_ext\": \"Tiempo de Extinci√≥n (min)\"},\n",
    "    text_auto=True,\n",
    ")\n",
    "\n",
    "# Rotar etiquetas del eje X para mejor visibilidad\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variables a analizar con boxplots\n",
    "variables_boxplot = [\"superficie\", \"time_ext\", \"time_ctrl\", \"muertos\", \"heridos\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "\n",
    "# Crear gr√°ficos de boxplot para cada variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(20, 4))\n",
    "fig.suptitle(\"Detecci√≥n de Outliers en Variables Clave\", fontsize=16)\n",
    "\n",
    "# Dibujar cada boxplot\n",
    "for ax, variable in zip(axes.flatten(), variables_boxplot):\n",
    "    sns.boxplot(y=df[variable], ax=ax)\n",
    "\n",
    "# Ajustar el layout para mejor visualizaci√≥n\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a transformar\n",
    "cols_to_transform = [\"superficie\", \"time_ext\", \"time_ctrl\", \"muertos\", \"heridos\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "\n",
    "\n",
    "# Aplicar la transformaci√≥n log(1 + x)\n",
    "df[cols_to_transform] = df[cols_to_transform].apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representaci√≥n box-plot para detecci√≥n de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variables a analizar con boxplots\n",
    "variables_boxplot = [\"superficie\", \"time_ext\", \"time_ctrl\", \"muertos\", \"heridos\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "\n",
    "# Crear gr√°ficos de boxplot para cada variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(20, 4))\n",
    "fig.suptitle(\"Detecci√≥n de Outliers en Variables Clave\", fontsize=16)\n",
    "\n",
    "# Dibujar cada boxplot\n",
    "for ax, variable in zip(axes.flatten(), variables_boxplot):\n",
    "    sns.boxplot(y=df[variable], ax=ax)\n",
    "\n",
    "# Ajustar el layout para mejor visualizaci√≥n\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los diagramas de caja y bigote de superficie (logaritmo) por causa\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x=\"causa_map\", y=\"superficie\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los diagramas de caja y bigote de superficie (logaritmo) por causa\n",
    "\n",
    "plt.figure(figsize=(21, 9))\n",
    "sns.boxplot(data=df, x=\"comunidad\", y=\"superficie\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Selecci√≥n de variables predictoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo columnas de inter√©s para el an√°lisis de correlaci√≥n\n",
    "variables_corr = [\"superficie\", \"time_ext\", \"time_ctrl\", \"personal\", \"gastos\", \"perdidas\"]\n",
    "df_numeric = df[variables_corr].apply(pd.to_numeric, errors=\"coerce\")  # Convertir a num√©rico y forzar errores a NaN\n",
    "\n",
    "# Calcular la matriz de correlaci√≥n\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "# Crear el heatmap de correlaci√≥n\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Matriz de Correlaci√≥n entre Variables de Incendios\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las variables m√°s correlacionadas entre s√≠ son el tiempo de control y el tiempo de extinci√≥n, pero no est√°n altamente correlacionadas, por lo que decidimos usar ambas variables para entrenamiento de modelo. Adem√°s, conocer el tiempo de extinci√≥n y el tiempo de contro simult√°neamente nos puede indicar la magnitud de superficie quemada, ya que un incendio que ha tardado m√°s tiempo en extinguirse desde el momento que se considera controlado ha podido ser m√°s devastador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia de variables con modelos basados en √°rboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables predictoras y objetivo\n",
    "X = df[[\"time_ext\", \"time_ctrl\", \"personal\", \"gastos\", \"perdidas\"]]\n",
    "y = df[\"superficie\"]\n",
    "\n",
    "# Entrenar modelo Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Extraer importancia de variables\n",
    "importances = pd.DataFrame({\"Variable\": X.columns, \"Importancia\": rf.feature_importances_})\n",
    "importances = importances.sort_values(by=\"Importancia\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(importances[\"Variable\"], importances[\"Importancia\"], color=\"red\", alpha = 0.5, edgecolor=\"black\")\n",
    "plt.title(\"Importancia de Variables en el Modelo\")\n",
    "plt.xlabel(\"Variable\")\n",
    "plt.ylabel(\"Importancia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizar pruebas estad√≠sticas para seleccionar las mejores caracter√≠sticas con K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar prueba estad√≠stica\n",
    "selector = SelectKBest(score_func=f_regression, k=3)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Ver qu√© variables fueron seleccionadas\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Entrenamiento red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Divisi√≥n del conjunto en entrenamiento y test (con el fin de evaluar nuestro regresor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ello, vamos a usar la librer√≠a de sklearn train_test_split.\n",
    "\n",
    "# En primer lugar nos quedamos con las variables predictoras y la variable objetivo\n",
    "\n",
    "X = df[['time_ext', 'time_ctrl', 'personal', 'perdidas', 'idcomunidad']]\n",
    "# X = df[['idcomunidad','causa','time_ext', 'time_ctrl', 'personal', 'gastos', 'perdidas']]\n",
    "y = df['superficie']\n",
    "\n",
    "# Dividimos los datos en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Codificaci√≥n de las variables categ√≥ricas y normalizaci√≥n de las variables num√©ricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categ√≥ricas y num√©ricas\n",
    "categorical_features = ['idcomunidad']#['causa']#, 'idcomunidad']\n",
    "numerical_features = ['time_ext', 'time_ctrl', 'personal', 'perdidas']\n",
    "\n",
    "# Crear pipeline con One-Hot Encoding y Normalizaci√≥n\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),  # Normalizar variables num√©ricas\n",
    "    ('cat', OneHotEncoder(drop='first'), categorical_features)  # One-Hot Encoding en categ√≥ricas\n",
    "])\n",
    "\n",
    "# Transformar los datos\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape, X_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Uso de cross-validation para calcular los hiperpar√°metros √≥ptimos de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la red neuronal base\n",
    "mlp = MLPRegressor(max_iter=500, random_state=42)\n",
    "\n",
    "# Definir los hiperpar√°metros a optimizar\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(24,), (32,), (32, 16)],  # Diferentes arquitecturas\n",
    "    'activation': ['relu', 'tanh'],  # Funciones de activaci√≥n\n",
    "    'solver': ['adam'],  # Algoritmos de optimizaci√≥n\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # Regularizaci√≥n L2 (distribuci√≥n uniforme)\n",
    "    'learning_rate': ['constant', 'adaptive'],  # Tipo de tasa de aprendizaje\n",
    "}\n",
    "\n",
    "\n",
    "# Ejecutar la b√∫squeda con Cross-Validation\n",
    "grid_search = GridSearchCV(mlp, \n",
    "                            param_grid,\n",
    "                             cv=5, # Cross-validation con 5 folds\n",
    "                             scoring='neg_mean_squared_error',  # Minimizar MAE\n",
    "                             n_jobs=-1, # Usar todos los n√∫cleos disponibles\n",
    "                             verbose=2) # Mostrar progreso\n",
    "\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Imprimir los mejores par√°metros encontrados\n",
    "print(\"Mejores hiperpar√°metros:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(X_test_transformed)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"üìâ MAE: {mae:.2f} hect√°reas\")\n",
    "print(f\"üìâ MSE: {mse:.2f} hect√°reas¬≤\")\n",
    "print(f\"üìä R¬≤ Score: {r2:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a entrenar una red neuronal sola\n",
    "\n",
    "# Crear la red neuronal\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(32,16),  # 2 capas ocultas con 64 y 32 neuronas\n",
    "                    activation='tanh',  # Funci√≥n de activaci√≥n ReLU\n",
    "                    solver='adam',  # Optimizador Adam\n",
    "                    alpha=0.0001,  # Regularizaci√≥n L2\n",
    "                    learning_rate='constant',  # Ajuste din√°mico de tasa de aprendizaje\n",
    "                    max_iter=500,  # N√∫mero de iteraciones de entrenamiento\n",
    "                    random_state=42)\n",
    "\n",
    "# Entrenar la red\n",
    "mlp.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predicciones en datos de prueba\n",
    "y_pred = mlp.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Evaluaci√≥n del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"üìâ MAE (Error Absoluto Medio): {mae:.2f} hect√°reas\")\n",
    "print(f\"üìâ MSE (Error Cuadr√°tico Medio): {mse:.2f} hect√°reas¬≤\")\n",
    "print(f\"üìä R¬≤ Score (Precisi√≥n del modelo): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veamos las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.element import Template, MacroElement\n",
    "\n",
    "\n",
    "color = ['yellow', 'blue', 'blue', 'red', 'purple', 'orange']\n",
    "# Definir umbral de error (en hect√°reas)\n",
    "umbral_error = 0  # Modificar seg√∫n necesidad\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'Real': y_test,\n",
    "    'Predicho': y_pred\n",
    "})\n",
    "\n",
    "# Calcular el error absoluto\n",
    "resultados['Error'] = np.abs(resultados['Real'] - resultados['Predicho'])\n",
    "\n",
    "resultados['id'] = X_test.index  # Asigna el √≠ndice de X_test como identificador\n",
    "df['id'] = df.index  # Si df tampoco tiene id, usa su √≠ndice\n",
    "\n",
    "# üìå Asegurar que df tiene 'id', 'lat', y 'lng' y hacer la uni√≥n correctamente\n",
    "resultados = resultados.merge(df, on='id', how='left')  # Cambia 'id' si el nombre es distinto\n",
    "\n",
    "# Filtrar puntos con error superior al umbral\n",
    "errores_significativos = resultados[resultados['Error'] > umbral_error]\n",
    "\n",
    "# Soluci√≥n: Eliminar NaN antes de usar Folium\n",
    "errores_significativos = errores_significativos.dropna(subset=['lat', 'lng'])\n",
    "\n",
    "# Crear mapa centrado en la media de los puntos\n",
    "mapa = folium.Map(location=[errores_significativos['lat'].mean(), \n",
    "                            errores_significativos['lng'].mean()], zoom_start=6)\n",
    "\n",
    "\n",
    "# Agrupar los marcadores\n",
    "marker_cluster = MarkerCluster().add_to(mapa)\n",
    "\n",
    "\n",
    "# Agregar puntos al mapa\n",
    "for _, fila in errores_significativos.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[fila['lat'], fila['lng']],\n",
    "        radius=fila['superficie'],\n",
    "        color=color[fila[\"causa\"]-1],\n",
    "        fill=True,\n",
    "        fill_color=color[fila[\"causa\"]-1],\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Real: {np.exp(fila['Real'])-1:.2f} ha \\n Predicho: {np.exp(fila['Predicho'])-1:.2f} ha \\n Error: {fila['Error']:.2f} ha \\n Causa: {fila['causa_map']}\"\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "# Mostrar el mapa interactivo\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa.save(\"mapa_interactivo_predicciones.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapa interactivo de los incendios cuya predicci√≥n difiere 19 hect√°reas del valor real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.element import Template, MacroElement\n",
    "\n",
    "\n",
    "color = ['yellow', 'blue', 'blue', 'red', 'purple', 'orange']\n",
    "# Definir umbral de error (en hect√°reas)\n",
    "umbral_error = 3  # Modificar seg√∫n necesidad\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'Real': y_test,\n",
    "    'Predicho': y_pred\n",
    "})\n",
    "\n",
    "# Calcular el error absoluto\n",
    "resultados['Error'] = np.abs(resultados['Real'] - resultados['Predicho'])\n",
    "\n",
    "resultados['id'] = X_test.index  # Asigna el √≠ndice de X_test como identificador\n",
    "df['id'] = df.index  # Si df tampoco tiene id, usa su √≠ndice\n",
    "\n",
    "# üìå Asegurar que df tiene 'id', 'lat', y 'lng' y hacer la uni√≥n correctamente\n",
    "resultados = resultados.merge(df, on='id', how='left')  # Cambia 'id' si el nombre es distinto\n",
    "\n",
    "# Filtrar puntos con error superior al umbral\n",
    "errores_significativos = resultados[resultados['Error'] > umbral_error]\n",
    "\n",
    "# Soluci√≥n: Eliminar NaN antes de usar Folium\n",
    "errores_significativos = errores_significativos.dropna(subset=['lat', 'lng'])\n",
    "\n",
    "# Crear mapa centrado en la media de los puntos\n",
    "mapa = folium.Map(location=[errores_significativos['lat'].mean(), \n",
    "                            errores_significativos['lng'].mean()], zoom_start=6)\n",
    "\n",
    "\n",
    "# Agrupar los marcadores\n",
    "marker_cluster = MarkerCluster().add_to(mapa)\n",
    "\n",
    "\n",
    "# Agregar puntos al mapa\n",
    "for _, fila in errores_significativos.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[fila['lat'], fila['lng']],\n",
    "        radius=fila['superficie'],\n",
    "        color=color[fila[\"causa\"]-1],\n",
    "        fill=True,\n",
    "        fill_color=color[fila[\"causa\"]-1],\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Real: {np.exp(fila['Real'])-1:.2f} ha \\n Predicho: {np.exp(fila['Predicho'])-1:.2f} ha \\n Error: {fila['Error']:.2f} ha \\n Causa: {fila['causa_map']}\"\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "# Mostrar el mapa interactivo\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa.save(\"mapa_interactivo_predicciones_3.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
